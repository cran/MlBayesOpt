<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Yuya Matsumura" />

<meta name="date" content="2019-03-20" />

<title>MlBayesOpt</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">MlBayesOpt</h1>
<h4 class="author"><em>Yuya Matsumura</em></h4>
<h4 class="date"><em>2019-03-20</em></h4>



<div id="overview" class="section level2">
<h2>Overview</h2>
<p>This is an R package to tune hyperparameters for machine learning algorithms using Bayesian Optimization based on Gaussian Processes. Algorithms currently supported are: Support vector machines, Random forest, and XGboost.</p>
<p>This package has some features:</p>
<ul>
<li>It’s very easy to write Bayesian Optimaization function, but you also able to customise your model very easily.</li>
<li>Any class (character, integer, factor) of label column is OK.</li>
</ul>
<div id="hyperprameter-tuning-in-machine-learning" class="section level3">
<h3>Hyperprameter Tuning in Machine Learning</h3>
<p>In many methods of machinelearning, it is very important to tune hyperparameters. “Grid Search” was often used to search the appropriate hyperaprameters, but it takes too much time to compute.</p>
<p>To solve this problem, Bayesian Optimization is often used to tune hyperparameters fast. This is a sequential design strategy for global optimization of black-box functions.</p>
<p>While Grid Search is simply an exhaustive searching through a manually specified subset of the hyperparameter space, Bayesian Optimization constructs a posterior distribution of functions (gaussian process) that describes the function you want to optimize best, and search the point whose score may be better.</p>
</div>
<div id="machine-learning-and-bayesian-optimization-in-r" class="section level3">
<h3>Machine Learning and Bayesian Optimization in R</h3>
<p>We could execute bayesian optimization using <code>rBayesianOptimization</code> package in the past.</p>
<ol start="0" style="list-style-type: decimal">
<li>Make data</li>
<li>Make the function to maximize</li>
<li>Execute the Bayesian Optimization</li>
</ol>
<p>For example, if you want to tune hyperparameters of XGboost with 5-fold cross validation, you have to write as following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(xgboost)
<span class="kw">library</span>(Matrix)

<span class="kw">data</span>(agaricus.train, <span class="dt">package =</span> <span class="st">&quot;xgboost&quot;</span>)
dtrain &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(agaricus.train<span class="op">$</span>data,
                      <span class="dt">label =</span> agaricus.train<span class="op">$</span>label)
cv_folds &lt;-<span class="st"> </span><span class="kw">KFold</span>(agaricus.train<span class="op">$</span>label, <span class="dt">nfolds =</span> <span class="dv">5</span>,
                  <span class="dt">stratified =</span> <span class="ot">TRUE</span>, <span class="dt">seed =</span> <span class="dv">0</span>)
xgb_cv_bayes &lt;-<span class="st"> </span><span class="cf">function</span>(max_depth, min_child_weight, subsample) {
  cv &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(<span class="dt">params =</span> <span class="kw">list</span>(<span class="dt">booster =</span> <span class="st">&quot;gbtree&quot;</span>, <span class="dt">eta =</span> <span class="fl">0.01</span>,
                             <span class="dt">max_depth =</span> max_depth,
                             <span class="dt">min_child_weight =</span> min_child_weight,
                             <span class="dt">subsample =</span> subsample, <span class="dt">colsample_bytree =</span> <span class="fl">0.3</span>,
                             <span class="dt">lambda =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="dv">0</span>,
                             <span class="dt">objective =</span> <span class="st">&quot;binary:logistic&quot;</span>,
                             <span class="dt">eval_metric =</span> <span class="st">&quot;auc&quot;</span>),
               <span class="dt">data =</span> dtrain, <span class="dt">nround =</span> <span class="dv">100</span>,
               <span class="dt">folds =</span> cv_folds, <span class="dt">prediction =</span> <span class="ot">TRUE</span>, <span class="dt">showsd =</span> <span class="ot">TRUE</span>,
               <span class="dt">early_stopping_rounds =</span> <span class="dv">5</span>, <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">verbose =</span> <span class="dv">0</span>)
  <span class="kw">list</span>(<span class="dt">Score =</span> cv<span class="op">$</span>evaluation_log<span class="op">$</span>test_auc_mean[cv<span class="op">$</span>best_iteration],
       <span class="dt">Pred =</span> cv<span class="op">$</span>pred)
}
OPT_Res &lt;-<span class="st"> </span><span class="kw">BayesianOptimization</span>(xgb_cv_bayes,
                                <span class="dt">bounds =</span> <span class="kw">list</span>(<span class="dt">max.depth =</span> <span class="kw">c</span>(2L, 6L),
                                              <span class="dt">min_child_weight =</span> <span class="kw">c</span>(1L, 10L),
                                              <span class="dt">subsample =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.8</span>)),
                                <span class="dt">init_grid_dt =</span> <span class="ot">NULL</span>, <span class="dt">init_points =</span> <span class="dv">10</span>, <span class="dt">n_iter =</span> <span class="dv">20</span>,
                                <span class="dt">acq =</span> <span class="st">&quot;ucb&quot;</span>, <span class="dt">kappa =</span> <span class="fl">2.576</span>, <span class="dt">eps =</span> <span class="fl">0.0</span>,
                                <span class="dt">verbose =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>On the other hand, we can write this very easily with <code>MlBayesOpt</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MlBayesOpt)

res0 &lt;-<span class="st"> </span><span class="kw">xgb_cv_opt</span>(<span class="dt">data =</span> agaricus.train<span class="op">$</span>data,
                   <span class="dt">label =</span> agaricus.train<span class="op">$</span>label,
                   <span class="dt">objectfun =</span> <span class="st">&quot;binary:logistic&quot;</span>,
                   <span class="dt">evalmetric =</span> <span class="st">&quot;auc&quot;</span>,
                   <span class="dt">n_folds =</span> <span class="dv">5</span>,
                   <span class="dt">acq =</span> <span class="st">&quot;ucb&quot;</span>,
                   <span class="dt">init_points =</span> <span class="dv">10</span>,
                   <span class="dt">n_iter =</span> <span class="dv">20</span>)</code></pre></div>
<p>When the data has <code>data.frame</code> class, you have only to write column name to specify the label.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># This takes a lot of time</span>
<span class="co"># fashion data is included in this package</span>
res0 &lt;-<span class="st"> </span><span class="kw">xgb_cv_opt</span>(<span class="dt">data =</span> fashion,
                   <span class="dt">label =</span> y,
                   <span class="dt">objectfun =</span> <span class="st">&quot;multi:softmax&quot;</span>,
                   <span class="dt">evalmetric =</span> <span class="st">&quot;merror&quot;</span>,
                   <span class="dt">n_folds =</span> <span class="dv">15</span>,
                   <span class="dt">classes =</span> <span class="dv">10</span>)</code></pre></div>
</div>
</div>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>You can install <strong>MlBayesOpt</strong> from CRAN:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">&quot;MlBayesOpt&quot;</span>)</code></pre></div>
<p>You can install MlBayesOpt (latest dev version) from github with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install.packages(&quot;githubinstall&quot;)</span>
githubinstall<span class="op">::</span><span class="kw">githubinstall</span>(<span class="st">&quot;MlBayesOpt&quot;</span>)

<span class="co"># install.packages(&quot;devtools&quot;)</span>
devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;ymattu/MlBayesOpt&quot;</span>)</code></pre></div>
<p>To use this package, please load it by <code>library()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MlBayesOpt)</code></pre></div>
<p>The source code for <strong>MlBayesOpt</strong> package is available on GitHub at</p>
<ul>
<li><a href="https://github.com/ymattu/MlBayesOpt" class="uri">https://github.com/ymattu/MlBayesOpt</a></li>
</ul>
</div>
<div id="details" class="section level2">
<h2>Details</h2>
<p>First of all, <code>*_opt()</code> functions mean “Hold-Out”, and <code>*_cv_opt()</code> ones mean “Cross Validation”. In “Hold Out” functions, you at least need to specify both the nameof data and the name of label column in training and test data. In “Cross Validation” functions, you have only to write column name to specify the label and how many times you validate in <code>n_folds</code> option.</p>
<p>Second, you can specify the options of Bayesian Optimization parameters, such as <code>init_points</code> or <code>n_iter</code>. For details of these options, see the help of <code>rBayesianOptimization::BayesianOptimization()</code> function.</p>
<p>Except for <code>xgb_cv_opt()</code> function, the returned value means test accuracy. In <code>xgb_cv_opt()</code> function, he returned value means the value you specified in <code>evalmetric</code> option.</p>
<div id="support-vector-machine" class="section level3">
<h3>Support Vector Machine</h3>
<p>About SVM, this package supports hold-out tuning (<code>svm_opt()</code>) and cross-validation tuning(<code>svm_cv_opt()</code>).</p>
<p>In SVM functions, you can specify the kind of kernel to compute (default is “radial”) from following options.</p>
<ul>
<li><strong>linear:</strong> <span class="math inline">\(u'v\)</span></li>
<li><strong>polynomial:</strong> <span class="math inline">\((\gamma u'v +coef0)^{degree}\)</span></li>
<li><strong>radial basis:</strong> <span class="math inline">\(\exp(-\gamma|u-v|^2)\)</span></li>
<li><strong>sigmoid:</strong> <span class="math inline">\(tanh(\gamma u'v + coef0)\)</span></li>
</ul>
<p>Also, when you want to adjust the range of parameters, you can.</p>
<p>For details of these SVM options, see the help of <code>e1071::svm()</code> function.</p>
<div id="example" class="section level4">
<h4>Example</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res0 &lt;-<span class="st"> </span><span class="kw">svm_cv_opt</span>(<span class="dt">data =</span> fashion_train,
                   <span class="dt">label =</span> y,
                   <span class="dt">svm_kernel =</span> <span class="st">&quot;polynomial&quot;</span>,
                   <span class="dt">degree_range =</span> <span class="kw">c</span>(2L, 4L),
                   <span class="dt">n_folds =</span> <span class="dv">3</span>,
                   <span class="dt">kappa =</span> <span class="dv">5</span>,
                   <span class="dt">init_points =</span> <span class="dv">4</span>,
                   <span class="dt">n_iter =</span> <span class="dv">5</span>)</code></pre></div>
</div>
</div>
<div id="random-forest" class="section level3">
<h3>Random Forest</h3>
<p>This package supports only hold-out tuning so far about Random Forest(<code>rf_opt()</code>).</p>
<p>In <code>rf_opt()</code>, you can specify the range of the number of the trees (<code>num_trees</code>), the value of mtry used (<code>mtry_range</code>), and the range of minimum node sizes to best tested (<code>min_node_size_range</code>).</p>
<p>For details of these Random Forest options, see the help of <code>ranger::ranger()</code> function.</p>
<div id="example-1" class="section level4">
<h4>Example</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res0 &lt;-<span class="st"> </span><span class="kw">rf_opt</span>(<span class="dt">train_data =</span> fashion_train,
               <span class="dt">train_label =</span> y,
               <span class="dt">test_data =</span> fashion_test,
               <span class="dt">test_label =</span> y,
               <span class="dt">mtry_range =</span> <span class="kw">c</span>(1L, <span class="kw">ncol</span>(fashion_train)<span class="op">-</span><span class="dv">1</span>),
               <span class="dt">num_tree =</span> 10L,
               <span class="dt">init_points =</span> <span class="dv">4</span>,
               <span class="dt">n_iter =</span> <span class="dv">5</span>)</code></pre></div>
</div>
</div>
<div id="xgboost" class="section level3">
<h3>XGboost</h3>
<p>For XGboost, this package supports hold-out tuning (<code>xgb_opt()</code>) and cross-validation tuning(<code>xgb_cv_opt()</code>).</p>
<p>In XGboost functions, you must specify object function like <code>objectfun = &quot;multi:softmax&quot;</code> and how to evaluate (e.g. <code>evalmetric = &quot;merror&quot;</code>). Also, you can specify the range of xgboost options like eta (<code>eta_range</code>) and subsample (<code>subsample_range</code>).</p>
<p>In only <code>xgb_cv_opt()</code> function, the returned value means the value you specified in <code>evalmetric</code> option, so when you specify the evaluation metric “merror” or “(m)logloss”, the value is minus.</p>
<p>For details of these Random Forest options, see the help of <code>xgboost::xgb.train()</code> or <code>xgboost::xgb.cv()</code> function.</p>
<div id="example-2" class="section level4">
<h4>Example</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res0 &lt;-<span class="st"> </span><span class="kw">xgb_cv_opt</span>(<span class="dt">data =</span> fashion_train,
                   <span class="dt">label =</span> y,
                   <span class="dt">objectfun =</span> <span class="st">&quot;multi:softmax&quot;</span>,
                   <span class="dt">evalmetric =</span> <span class="st">&quot;merror&quot;</span>,
                   <span class="dt">n_folds =</span> <span class="dv">3</span>,
                   <span class="dt">classes =</span> <span class="dv">10</span>,
                   <span class="dt">init_points =</span> <span class="dv">4</span>,
                   <span class="dt">n_iter =</span> <span class="dv">5</span>)</code></pre></div>
</div>
</div>
</div>
<div id="related-works" class="section level2">
<h2>Related Works</h2>
<ul>
<li><a href="https://github.com/yanyachen/rBayesianOptimization">rBayesianOptimization</a></li>
<li><a href="https://cran.r-project.org/package=e1071">e1071</a></li>
<li><a href="https://github.com/imbs-hl/ranger">ranger</a></li>
<li><a href="https://github.com/dmlc/xgboost">xgboost</a></li>
</ul>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
